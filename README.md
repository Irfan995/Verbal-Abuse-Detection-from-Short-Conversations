## ğŸ“˜ Overview

Smart assistants and smart microphones can contribute to detecting verbal abuse by analyzing speech-to-text contents. In this paper, we compare different **large language models (LLMs)** for detecting verbal abuse and propose a framework that first identifies emotion from short audio conversations by extracting **Mel Frequency Cepstral Coefficient (MFCC) and Mel Spectrogram (MEL)** features. It then utilizes **transfer learning** with a fully connected neural network incorporating an attention mechanism and **SBERT** encoding. To demonstrate the efficacy of the proposed framework, we prepared a custom dataset containing instances of verbal abuse. Evaluation results show that our framework is lightweight and achieves commendable accuracy compared to the existing LLM models.

The repository includes:
- **`multi_label_saf.py`** â€” core model training and evaluation  
- **`k_folded_context_detection_saf.py`** â€” K-Fold cross-validation framework for performance reliability  
- **`Emotions_All_Data.csv`** â€” dataset containing multi-emotion annotations  
- **`family_conflict_conv.xlsx`** â€” conversational transcripts for contextual understanding  

---

## âš™ï¸ Features

- Multi-label classification (each utterance may express several emotions)
- Sentence-level semantic embeddings using **SBERT/BERT**
- Attention mechanism for contextual fusion
- K-Fold validation to ensure generalization
- Metrics: Precision, Recall, F1-score
- End-to-end preprocessing and tokenization pipeline
- Easily extendable for new emotion taxonomies or datasets

---

## ğŸ§© Project Structure

```
.
â”œâ”€â”€ multi_label_saf.py                 # Main SAF training script
â”œâ”€â”€ k_folded_context_detection_saf.py  # K-Fold validation script
â”œâ”€â”€ family_conflict_conv.xlsx          # Conversation dataset
â”œâ”€â”€ Emotions_All_Data.csv              # Emotion-labeled dataset
â””â”€â”€ README.md
```

---

## ğŸ§  Model Architecture

The **Smart Annotation Framework (SAF)** model integrates:
1. **Sentence Embeddings:** obtained from Sentence-BERT or a similar transformer encoder  
2. **Attention Layer:** captures contextual dependencies between utterances  
3. **Dense Layers:** project contextualized embeddings into the multi-label output space  

```python
# Simplified example
encoded = SentenceTransformer('all-mpnet-base-v2').encode(sentences)
contextual = AttentionLayer()(encoded)
output = Dense(num_labels, activation='sigmoid')(contextual)
```

---

## ğŸ§ª Datasets

| Dataset | Description | Format | Labels |
|----------|-------------|--------|---------|
| `Emotions_All_Data.csv` | Emotion dataset with multiple labels per utterance | CSV | Joy, Sadness, Anger, Fear, Disgust, Surprise |
| `family_conflict_conv.xlsx` | Multi-turn family conflict dialogues | XLSX | Contextual and emotional turns |

---

## ğŸš€ Usage

### 1ï¸âƒ£ Installation

```bash
git clone https://github.com/<your-username>Verbal-Abuse-Detection-from-Short-Conversations.git
cd multi-label-saf
pip install -r requirements.txt
```

### 2ï¸âƒ£ Train the Model

```bash
python multi_label_saf.py
```

### 3ï¸âƒ£ Run K-Fold Cross-Validation

```bash
python k_folded_context_detection_saf.py
```

### 4ï¸âƒ£ Evaluate Results

Metrics are automatically saved and printed:
- Precision / Recall / F1
- Label-wise ROC curves
- K-Fold averaged performance

---

## ğŸ“Š Example Output

| Fold | Precision | Recall | F1-Score |
|------|------------|--------|----------|
| 1 | 0.84 | 0.82 | 0.83 | 
| 2 | 0.86 | 0.83 | 0.84 | 
| 3 | 0.85 | 0.84 | 0.85 | 

---

## ğŸ§° Requirements

- Python â‰¥ 3.8  
- TensorFlow / Keras â‰¥ 2.12  
- scikit-learn â‰¥ 1.3  
- pandas, numpy, matplotlib  
- sentence-transformers  

Install dependencies:

```bash
pip install tensorflow scikit-learn sentence-transformers pandas numpy matplotlib
```

---

## ğŸ§© Extending the Framework

To adapt the framework for a new dataset:
1. Place the dataset file (CSV/XLSX) in the project root.  
2. Update the `DATASET_PATH` variable in `multi_label_saf.py`.  
3. Modify `LABEL_COLUMNS` to match your new annotation scheme.  
4. Run the training script â€” the pipeline automatically preprocesses and trains.

---

## ğŸ“– Citation

Please cite the following paper:

> F. A. Irfan, C. Behl and R. Iqbal, "Verbal Abuse Detection from Short Conversations," *2025 IEEE 22nd Consumer Communications & Networking Conference (CCNC)*, Las Vegas, NV, USA, 2025, pp. 1â€“2.  
> DOI: [https://doi.org/10.1109/CCNC54725.2025.10976181](https://doi.org/10.1109/CCNC54725.2025.10976181)

**BibTeX:**
```bibtex
@inproceedings{irfan2025verbal,
  author    = {F. A. Irfan and C. Behl and R. Iqbal},
  title     = {Verbal Abuse Detection from Short Conversations},
  booktitle = {2025 IEEE 22nd Consumer Communications \& Networking Conference (CCNC)},
  year      = {2025},
  pages     = {1--2},
  address   = {Las Vegas, NV, USA},
  doi       = {10.1109/CCNC54725.2025.10976181},
  keywords  = {Emotion recognition; Attention mechanisms; Accuracy; Transfer learning; Neural networks; Oral communication; Bidirectional control; Feature extraction; Encoding; Mel frequency cepstral coefficient; Attention mechanism; BERT; Emotion detection; K-fold validation; Neural network; SBERT; Transfer learning}
}
```

If you use this dataset or code:
> DOI: [10.5281/zenodo.17445624](https://doi.org/10.5281/zenodo.17445624)

---


## ğŸªª License

This project is released under the **MIT License**.  
See [LICENSE](LICENSE) for more details.

---

